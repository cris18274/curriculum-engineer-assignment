{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dbd02b-ab2e-42e6-90d9-10e92ee54853",
   "metadata": {},
   "source": [
    "# Forward Propagation Exercise\n",
    "\n",
    "In this exercise, you'll implement **forward propagation** for a simple neural network. Forward propagation is the process of computing the network's output given an input by passing data through the layers.\n",
    "\n",
    "## Neural Network Architecture\n",
    "- **Input Layer**: 3 features\n",
    "- **Hidden Layer**: 4 neurons with ReLU activation\n",
    "- **Output Layer**: 1 neuron with sigmoid activation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how to compute the output of a neural network using matrix operations.\n",
    "- Apply ReLU and sigmoid activation functions.\n",
    "- Store intermediate values in a cache for later use (e.g., in backpropagation).\n",
    "\n",
    "## Instructions\n",
    "- Complete the `forward_propagation` function by filling in the placeholders marked `# your code here`.\n",
    "- Use `np.dot` for matrix multiplication, `relu` for the hidden layer activation, and `sigmoid` for the output layer activation.\n",
    "- Run the unit test cell to verify your implementation.\n",
    "- The cache dictionary should store `Z1`, `A1`, `Z2`, and `A2` for use in backpropagation.\n",
    "\n",
    "**Tip**: Ensure matrix shapes align during computations (e.g., `W1` is (4, 3), `X` is (3, m), so `np.dot(W1, X)` produces (4, m))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450798a-e946-4deb-94f7-78ec93b50f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid activation function.\n",
    "    Arguments:\n",
    "        z -- Input array\n",
    "    Returns:\n",
    "        Sigmoid of z (values between 0 and 1)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Compute the ReLU activation function.\n",
    "    Arguments:\n",
    "        z -- Input array\n",
    "    Returns:\n",
    "        ReLU of z (values >= 0)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfa2aa-f51a-46d3-b250-b5985e252f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input data of size (n_x, m), where n_x is the number of features and m is the number of examples\n",
    "    parameters -- dictionary containing weights and biases:\n",
    "                  W1 -- weight matrix of shape (4, 3)\n",
    "                  b1 -- bias vector of shape (4, 1)\n",
    "                  W2 -- weight matrix of shape (1, 4)\n",
    "                  b2 -- bias vector of shape (1, 1)\n",
    "\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation, shape (1, m)\n",
    "    cache -- a dictionary containing Z1, A1, Z2, A2 for later use\n",
    "    \"\"\"\n",
    "    # Retrieve parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    Z1 = # your code here (Linear transformation: W1*X + b1)\n",
    "    A1 = # your code here (Apply ReLU to Z1)\n",
    "    Z2 = # your code here (Linear transformation: W2*A1 + b2)\n",
    "    A2 = # your code here (Apply sigmoid to Z2)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f12d8-d061-4ea9-aea7-ce1d55e6335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_propagation():\n",
    "    \"\"\"\n",
    "    Unit test to verify the forward_propagation function.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    X = np.random.randn(3, 2)\n",
    "    parameters = {\n",
    "        \"W1\": np.random.randn(4, 3),\n",
    "        \"b1\": np.random.randn(4, 1),\n",
    "        \"W2\": np.random.randn(1, 4),\n",
    "        \"b2\": np.random.randn(1, 1)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        assert A2.shape == (1, 2), f\"Expected A2 shape (1, 2), got {A2.shape}\"\n",
    "        assert cache[\"Z1\"].shape == (4, 2), f\"Expected Z1 shape (4, 2), got {cache['Z1'].shape}\"\n",
    "        assert cache[\"A1\"].shape == (4, 2), f\"Expected A1 shape (4, 2), got {cache['A1'].shape}\"\n",
    "        assert cache[\"Z2\"].shape == (1, 2), f\"Expected Z2 shape (1, 2), got {cache['Z2'].shape}\"\n",
    "        assert np.allclose(A2, np.array([[0.11046056, 0.14411428]]), atol=1e-2), \"Unexpected A2 values\"\n",
    "        print(\"✅ All tests passed!\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7da5ad-fdef-478b-b666-b7c1944b6629",
   "metadata": {},
   "source": [
    "## Solution \n",
    "\n",
    "The following cell contains the complete implementation of the `forward_propagation` function. This is provided for instructors and is not visible to learners.\n",
    "\n",
    "```python\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- dictionary containing weights and biases:\n",
    "                  W1, b1, W2, b2\n",
    "\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing Z1, A1, Z2, A2\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1  # Linear transformation for hidden layer\n",
    "    A1 = relu(Z1)            # ReLU activation\n",
    "    Z2 = np.dot(W2, A1) + b2 # Linear transformation for output layer\n",
    "    A2 = sigmoid(Z2)         # Sigmoid activation\n",
    "\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "# Notes:\n",
    "# - The solution uses np.dot for matrix multiplication and adds biases correctly.\n",
    "# - ReLU ensures non-negative outputs for the hidden layer.\n",
    "# - Sigmoid maps the output to [0, 1], suitable for binary classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
